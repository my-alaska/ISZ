{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# FoCal - VAE approach\n",
    "\n",
    "### Set up data globals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb5bc31ad29e72c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "dataroot_path = DATA_DIR / \"InputsForML.root\"\n",
    "\n",
    "INPUT_SIZE = 3\n",
    "RESPONSE_SHAPE = (105, 105)\n",
    "N_ELEMENTS = 105 * 105\n",
    "\n",
    "RESPONSE_COLUMN = \"mCellsEnergy\"\n",
    "INPUT_COLUMNS = [\n",
    "    \"mPartEnergy\",\n",
    "    \"mPartEta\",\n",
    "    \"mPartPhi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read inputs and responses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "595d07f18cbdb5ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "with uproot.open(dataroot_path) as file:\n",
    "    tree = file[\"myTree;1\"]\n",
    "    fullsim = tree.arrays(library=\"pd\")\n",
    "\n",
    "fullsim.head(8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d84f899a7e690c50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8d88461089f3d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "responses = fullsim[RESPONSE_COLUMN].to_numpy(dtype=\"float32\")\n",
    "n_nans = np.isnan(responses).sum(axis=(1, 2)).shape[0]\n",
    "\n",
    "print(f\"Number of NaN values: {n_nans}\")\n",
    "\n",
    "responses[np.isnan(responses)] = 0\n",
    "responses[responses < 0] = 0\n",
    "responses[responses > 200] = 0\n",
    "responses += 1e-5\n",
    "responses = np.log(responses)\n",
    "\n",
    "scaler = MinMaxScaler((0.0, 1.0))\n",
    "\n",
    "responses = scaler.fit_transform(responses.reshape(-1, N_ELEMENTS)).reshape(\n",
    "    -1, *RESPONSE_SHAPE\n",
    ")\n",
    "\n",
    "responses = torch.tensor(responses, dtype=torch.float32)\n",
    "responses[responses > 1] = 1  # throws errors otherwise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ab7d69291f0d218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess inputs\n",
    "\n",
    "extract inputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1169219444c3646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs_raw = fullsim[INPUT_COLUMNS].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3afbb72747cbf910"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot input value distributions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "850fb9e7ff12cd0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(5, 3 * INPUT_SIZE))\n",
    "\n",
    "for i in range(INPUT_SIZE):\n",
    "    ax[i].hist(inputs_raw[:, i], bins=300)\n",
    "\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae9ea9f8377b5863"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scale inputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b23c4828cd545f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "input_scaler = StandardScaler()\n",
    "inputs = input_scaler.fit_transform(inputs_raw)\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2752bf863e61c504"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display example images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632ae2471aaf5a78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "def show_image_grid(\n",
    "    images,\n",
    "    nrow=4,\n",
    "    padding=2,\n",
    "    title=\"Samples\",\n",
    "    figsize=(10, 10),\n",
    "):\n",
    "    images = images.unsqueeze(1)\n",
    "    grid = vutils.make_grid(images, nrow=nrow, padding=padding, pad_value=1)\n",
    "    np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(np_grid[:, :, 0], cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_image_grid(responses[:25], 5, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f720c28c33f6d970"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preapare for training\n",
    "\n",
    "### Set up globals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed4e0a7890c5392"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "TRAIN_SIZE = 0.7\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e08da438fc652d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6bf8c498ed3fce5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_responses, test_responses, train_inputs, test_inputs = train_test_split(\n",
    "    responses, inputs, train_size=TRAIN_SIZE, random_state=42\n",
    ")\n",
    "\n",
    "valid_responses, test_responses, valid_inputs, test_inputs = train_test_split(\n",
    "    test_responses, test_inputs, train_size=0.5, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb34d2049e92bfcf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset objects and dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc14902035057ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_responses, train_inputs)\n",
    "valid_dataset = TensorDataset(valid_responses, valid_inputs)\n",
    "test_dataset = TensorDataset(test_responses, test_inputs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "704d46bb7743ae3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4ed07bdce5c945b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2),  # 105 -> 53\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),  # 53 -> 27\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),  # 27 -> 14\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256 * 14 * 14, 256 * 14),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 * 14, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened feature map after convolutions\n",
    "        flattened_size = 256\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(flattened_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flattened_size, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, flattened_size),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256 * 14),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 * 14, 256 * 14 * 14),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Unflatten(1, (256, 14, 14)),\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=5, stride=2, padding=2\n",
    "            ),  # 14 -> 27\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2),  # 27 -> 53\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=5, stride=2, padding=2),  # 53 -> 105\n",
    "            # nn.Sigmoid() # we use logit loss\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7886717f470958b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure training parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f09c5c587e4d5d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "latent_dim = 32\n",
    "learning_rate = 5e-4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b625cdeb2c88b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize model, optimizer and create the loss function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4603c2ec3a3457d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = VAE(latent_dim=latent_dim).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # I noticed that the model doesn't predict high pixel values as well as low pixel values\n",
    "    # The (x+1) modifier scales the loss to penalize large entropy for higher values more\n",
    "    BCE = torch.sum(\n",
    "        nn.functional.binary_cross_entropy_with_logits(recon_x, x, reduction=\"none\")\n",
    "        * (x + 1)\n",
    "    )\n",
    "    KLD = -torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / 2\n",
    "\n",
    "    return BCE, KLD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b2b3bea8a99fc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf0e7909efbc55f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_bce_list = []\n",
    "train_kld_list = []\n",
    "valid_bce_list = []\n",
    "valid_kld_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_bce = 0\n",
    "    train_kld = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (images, params) in enumerate(train_loader):\n",
    "        images = images.unsqueeze(1).to(DEVICE)  # Add channel dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(images)\n",
    "\n",
    "        bce_loss, kld_loss = loss_function(x_recon, images, mu, logvar)\n",
    "        bce_loss = bce_loss\n",
    "        kld_loss = kld_loss\n",
    "\n",
    "        train_bce += bce_loss.item()\n",
    "        train_kld += kld_loss.item()\n",
    "\n",
    "        loss = bce_loss + kld_loss\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    valid_bce = 0\n",
    "    valid_kld = 0\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, params) in enumerate(valid_loader):\n",
    "            images = images.unsqueeze(1).to(DEVICE)  # Add channel dimension\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(images)\n",
    "\n",
    "            bce_loss, kld_loss = loss_function(x_recon, images, mu, logvar)\n",
    "            bce_loss = bce_loss\n",
    "            kld_loss = kld_loss\n",
    "\n",
    "            valid_bce += bce_loss.item()\n",
    "            valid_kld += kld_loss.item()\n",
    "            loss = bce_loss + kld_loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    train_bce_list.append(train_bce)\n",
    "    train_kld_list.append(train_kld)\n",
    "    valid_bce_list.append(valid_bce)\n",
    "    valid_kld_list.append(valid_kld)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(\"train\")\n",
    "    print(f\" - BCE loss   : {train_bce / len(train_dataset):.4f}\")\n",
    "    print(f\" - KLD loss   : {train_kld / len(train_dataset):.4f}\")\n",
    "    print(f\" - total loss : {train_loss / len(train_dataset):.4f}\")\n",
    "    print(\"valid\")\n",
    "    print(f\" - BCE loss   : {valid_bce / len(valid_dataset):.4f}\")\n",
    "    print(f\" - KLD loss   : {valid_kld / len(valid_dataset):.4f}\")\n",
    "    print(f\" - total loss : {valid_loss / len(valid_dataset):.4f}\")\n",
    "    print(\"==========\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbeaadcc574127bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "ax[0].set_title(\"KLD losses\")\n",
    "ax[0].plot(train_kld_list, label=\"train\")\n",
    "ax[0].plot(valid_kld_list, label=\"valid\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title(\"BCE losses\")\n",
    "ax[1].plot(train_bce_list, label=\"train\")\n",
    "ax[1].plot(valid_bce_list, label=\"valid\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81db7599ef7b8a91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_recon, mu, logvar = model(test_dataset[3002:3003][0].unsqueeze(1).to(DEVICE))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea9dd40bd4a542b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "original = test_dataset[3002][0].squeeze().cpu().detach().numpy()\n",
    "reconstructed = nn.functional.sigmoid(x_recon[0]).squeeze().cpu().detach().numpy()\n",
    "\n",
    "ax[0].imshow(original)\n",
    "ax[0].set_title(\"original image\")\n",
    "ax[1].imshow(reconstructed)\n",
    "ax[1].set_title(\"reconstructed image\")\n",
    "\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef641561ecf29304"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "41617a7e59e634fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
